# Hyperparameter for Machine Learning Algorithms
experiment_name: "Titanic"
RANDOM_STATE: 42
N_TRAILS: 50 # run for xx runs
TIMEOUT: 600 # run for max 10 minutes (if the last run at 9 minutes runs for 1h, the active run is not killed)
N_SPLITS: 5
TEST_SIZE: 0.2


path_df_train: "../03_DataPreprocessing/df_train_prepared_reduced.pkl"
path_df_test: "../03_DataPreprocessing/df_test_prepared_reduced.pkl"

# SVC
svm_kernel: ['linear', 'poly', 'rbf', 'sigmoid']
svm_C: [1, 500]
svm_degree: [1, 5]

# Logistic Regression
lr_C: [1, 500]
lr_penalty: ['l2', 'l1']

# Decision Tree Classifier
dt_max_depth: [5]
dt_criterion: ['gini', 'entropy']
dt_max_leaf_nodes: [2, 10]

# Extra Trees Classifier
etc_n_estimators: [10, 5000]
etc_max_depth: [5]
etc_min_samples_split: [0.05, 0.2]
etc_min_samples_leaf: [0.05, 0.2]
etc_criterion: ['gini', 'entropy']
etc_max_leaf_nodes: [2, 10]

# Random Forest Classifier
rfc_n_estimators: [10, 5000]
rfc_max_depth: [5]
rfc_min_samples_split: [0.05, 0.2]
rfc_min_samples_leaf: [0.05, 0.2]
rfc_criterion: ['gini', 'entropy']
rfc_max_leaf_nodes: [2, 10]

# XGB Classifier
xgb_n_estimators: [10, 5000]
xgb_learning_rate: [0.01, 1]
xgb_reg_lambda: [0.000000001, 100]
xgb_reg_alpha: [0.000000001, 100]

# LGBM Classifier
lgb_n_estimators: [10, 5000]
lgb_learning_rate: [0.01, 10]
lgb_max_depth: [1, 16]
lgb_num_leaves: [2]
lgb_min_data_in_leaf: [0, 300]
lgb_subsample: [0.01, 1]
lgb_feature_fraction: [0.1, 1]
lgb_reg_lambda: [0.01, 100]
lgb_reg_alpha: [0.01, 100]

# Gradient Boosting Classifier
gbc_n_estimators: [10, 5000]
gbc_learning_rate: [0.01, 10]
gbc_max_depth: [1, 16]
gbc_min_samples_split: [0.05, 0.2]
gbc_min_samples_leaf: [0.05, 0.2]
gbc_max_leaf_nodes: [2, 10]
